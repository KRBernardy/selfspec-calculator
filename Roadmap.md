# Project Documentation: Self-Speculating Analog In-Memory Computing

## 1. Background & Architectural Evolution

This section details the transition from traditional digital-mimicking analog architectures to the proposed residual physics-based architecture.

### 1.1 Limitation of Conventional Architectures: Bit-Slicing

The standard industry approach to high-precision Analog Compute-in-Memory (ACIM) forces analog devices to mimic digital binary logic.

* **The Mechanism:** A weight (e.g., 8-bit integer) is sliced into multiple low-precision segments (e.g., four 2-bit slices). Each slice represents a strictly defined power of 2 ($2^0, 2^2, 2^4, 2^6$).
* **The Hardware Bottleneck:**
  * **Redundant Digitization:** Each slice requires its own ADC operation (or multiple cycles of a shared ADC).
  * **Digital Reconstruction Overhead:** The results must be digitally shifted and added. This consumes significant dynamic power and chip area for shift-adders.
  * **MSB Sensitivity:** The Most Significant Bit (MSB) arrays are unforgiving. A slight analog variation in the $2^6$ array translates to a massive error in the final output, requiring expensive error-correction codes (ECC) or extreme device endurance.

### 1.2 Proposed Base Architecture: Residual "True Analog"

We adopt the architecture proposed by *Song et al. (Science, 2024)*, which treats quantization noise and device write noise as a unified physical "Residual."

* **The Concept:** Instead of pre-slicing bits, we write values sequentially based on physical feedback.
    1. **Array 1 (Coarse)** attempts to write the target value $W$. Due to variability, it settles at $W_{actual}$.
    2. The system measures the error $\epsilon_1 = W - W_{actual}$.
    3. **Array 2 (Residual)** is programmed to store $\epsilon_1$, scaled by a gain factor $K$ to maximize the dynamic range of the device.
    4. This repeats for Arrays 3 and 4.
* **The Analog Advantage:** The summation of stored values occurs via **Kirchhoff’s Current Law (KCL)** on the bitline *before* the ADC involves.
    $$ I_{total} = I_{Array1} + (I_{Array2}/K) + (I_{Array3}/K^2) $$
* **Key Implication:** High precision ($10^{-15}$) is achieved with a single readout interface, removing the digital reconstruction overhead.

![Residual Architecture](./Idea%20Slide%20Figure/main.png)

---

## 2. Core Innovation: Zero-Overhead Speculative Decoding

Large Language Models (LLMs) do not require scientific-grade precision for every token. We leverage the physical properties of the Residual Architecture to implement **Speculative Decoding** natively in hardware, without the extra memory cost required by software speculative decoding (which needs a separate "Draft Model").

### 2.1 The "Hardware-Native" Draft Model

In the Residual Architecture, **Array 1** (the uncompensated array) is mathematically equivalent to the quantization of the Full Model plus a noise term.

* **Magnitude Dominance:** Array 1 typically captures >95% of the signal energy (the "shape" of the weight vector). Arrays 2-4 only provide the "fine tuning."
* **The Strategy:** We repurpose Array 1 as a standalone "Draft Model." By reading only Array 1, we obtain a fast, low-energy approximation of the next token.

### 2.2 Dual-Mode Inference Strategy

Software controls the hardware state to toggle between "High Throughput" and "High Precision."

#### Mode A: Draft Mode (High Speed)

* **Hardware State:** Switch 1 is Closed (Active). Switches 2-4 are Open (Power Gated).
* **ADC Config:** Low-Resolution (4-bit), Single-Shot sampling.
* **Latency:** Ultra-low (e.g., 5ns).
* **Usage:** Used to auto-regressively generate a sequence of $K$ future tokens (e.g., 5 tokens ahead).

#### Mode B: Verify Mode (High Precision)

* **Hardware State:** All Switches (1-4) are Closed. The current represents the full precision weight.
* **ADC Config:** High-Resolution (16-bit equivalent), Multi-sample averaging (CDS).
* **Latency:** High (e.g., 50ns).
* **Usage:** Used to validate the sequence generated by the Draft Mode.

### 2.3 Spatial Pipelined Verification

Why is this faster? Conventional analog inference is limited by **Setup Latency** (charging long capacitive bitlines).

* **Conventional (Stop-and-Go):** Setup -> Read Token 1 -> Setup -> Read Token 2. The setup penalty is paid $N$ times.
* **Speculative Burst:**
    1. We generate 5 tokens quickly using Draft Mode.
    2. We feed all 5 inputs into the High-Precision Verifier in a **Burst**.
    3. The bitlines remain charged/active. The ADC operates in "Throughput Mode" rather than "Latency Mode."
    4. The Setup penalty is paid only **once** for the entire burst.

---

## 3. Hardware Architecture Implementation

To maximize the efficiency of the Dual-Mode strategy, we redesign the peripheral readout circuitry (ADC Layout). The standard single-ADC design creates a data bottleneck; our proposed design enables reuse and parallelism.

### 3.1 Design Choice 1: The "1+3 Split" ADC Layout

We replace the single monolithic ADC with a decoupled, heterogeneous readout system.

* **Component A: ADC-Draft (4-bit Flash/SAR)**
  * Connected permanently to **Array 1**.
  * Speed: Very Fast (<1 cycle).
  * Power: Negligible.
* **Component B: ADC-Residual (12-bit Pipeline/SAR)**
  * Connected via switches to the sum of **Arrays 2, 3, and 4**.
  * Dynamic Range: Tuned for small currents (high gain).
* **The "Non-Destructive Reuse" Logic:**
  * During Draft Mode: Only **ADC-Draft** fires. Result is stored in a register ($D_{reg}$).
  * During Verify Mode: We **do not re-read Array 1**. We assume $D_{reg}$ is the base value. We activate Arrays 2-4, read them via **ADC-Residual** to get correction $C$, and digitally compute $Final = D_{reg} + C$.
  * **Energy Savings:** Removes the redundant active power of reading the heavy Array 1 during verification phases.

### 3.2 Design Choice 2: Predictive Delta Readout (Combinable with Opt 1)

LLM outputs exhibit high **Temporal Locality**. The voltage on a bitline for Token $N$ is statistically very close to Token $N-1$ (due to embedding similarity or attention stability).

* **Mechanism:**
    1. The digitized output of Token $N-1$ is stored in a Feedback Register.
    2. A **DAC (Digital-to-Analog Converter)** injects a negative current corresponding to Token $N-1$ onto the bitline during the read of Token $N$.
    3. **The Result:** The ADC bitline now holds $\Delta V = V_{token\_N} - V_{token\_N-1}$.
* **Gain:** The $\Delta V$ is much smaller than the absolute $V$. This signal compression allows us to lower the required dynamic range of **ADC-Residual**, achieving effective 16-bit precision using only 8-10 physical bits.

![Speculative Inference Timeline](./Idea%20Slide%20Figure/opt.png)

---

## 4. Algorithm-Hardware Co-Design (Optimizations)

Hardware alone cannot guarantee efficiency. If the "Draft Model" (Array 1) is too inaccurate, the speculation will fail, and we will revert to slow verification too often. We use two algorithmic techniques to maximize the **Draft Acceptance Rate**.

### 4.1 Optimization 1: Sensitivity-Aware Draft Allocation

Not all layers in a Transformer affect the output equally. Some layers act as "Memory Retrieval" (High Sensitivity), while others process robust features (Low Sensitivity).

* **Profiling:** We use Hessian-based sensitivity analysis (or magnitude pruning scores) to classify layers into **Robust** and **Sensitive**.
* **Dynamic Allocation Policy:**
  * **Robust Layers:** Draft using **Array 1 Only**. (Maximum Speed).
  * **Sensitive Layers:** Draft using **Array 1 + Array 2**, or maybe even use **All 4 Arrays**. (Sacrifice some speed for much higher draft accuracy).
* **Control Flow:** A lightweight controller looks up the "Precision ID" for the current layer index before triggering the Draft Burst.

### 4.2 Optimization 2: Noise-Injection Adaptation (Fine-Tuning)

A standard INT8 quantization model assumes perfect integer math. When deployed on **Array 1**, the analog write noise (Gaussian perturbation) causes "Golden" weights to drift, destroying the accuracy of the draft.

* **The Mismatch:** Standard models have sharp minima. A small analog drift pushes the weight out of the optimal valley, causing token flips.
* **The Solution:** Noise-Injection Training (NIT).
  * **Procedure:** We fine-tune the LLM for a small number of steps (e.g., 1000 steps).
  * **Injection:** During the Forward Pass, we add random noise $\mathcal{N}(0, \sigma_{device})$ to the weights, where $\sigma_{device}$ matches the measured physical write variability of the memristor.
  * **Result:** The optimizer seeks "Flat Minima"—regions where weight variations do not change the output logits.
* **Metric Improvement:** Increases Draft Acceptance Rate from ~60% (unusable) to >85% (highly efficient), enabling the speculative pipeline to maintain throughput.

## 5. Evaluation Methodology

To validate the proposed **Self-Speculating Analog Architecture**, we employ a hybrid simulation approach. Since the specific hardware (the Dual-ADC Analog SoC) utilizes novel readout logic not yet available in silicon, we decouple the evaluation into two distinct modules: a **Functional Simulator** (for accuracy and acceptance rates) and a **Hardware Estimator** (for energy, latency, and area).

### 5.1 Simulator Framework Overview

The verification pipeline consists of two interacting components:

1. **The Accuracy Engine (PyTorch):** Based on the open-source **`gpt-fast`** (by Meta PyTorch Team). We will modify this to simulate the algorithmic behavior, injecting noise to mimic the analog "Draft" and performing quantization to mimic the "Residuals."
2. **The Performance Calculator (Python):** A parametric tool that takes the usage statistics from the Accuracy Engine (e.g., "How many tokens were accepted?") and maps them to physical metrics using industry-standard hardware specifications.

### 5.2 Part A: Functional Simulation (Accuracy & Speculation)

We intend to modify the `gpt-fast` inference loop to emulate the physical behavior of the residual arrays.

* **Base Model:** **[Models around 1B, like Llama 3 1B, Qwen 2.5 1.5B or GPT-2-XL]**
* **Datasets:** **[PLACEHOLDER: e.g., WikiText-2, C4, or HumanEval]**

#### 5.2.1 Noise & Quantization Modeling

To simulate **Array 1 (The Draft Model)**, we apply the following transformation to the weight matrices $W$:

1. **Quantization:** $W_{q} = \text{Quantize}(W, \text{bits}=4)$.
2. **Analog Noise Injection:** $W_{draft} = W_{q} + \mathcal{N}(0, \sigma_{write})$.
    * $\sigma_{write}$ is derived from physical device measurements found in **[PLACEHOLDER: Measure data from Song et al. or 1T1R device paper]**.
3. **Residual Simulation:** The "Verification" step uses the original $W$ (FP16), assuming the Hardware Opt 1 (Arrays 2-4) successfully reconstructs the high-precision value.

#### 5.2.2 Speculation Logic Implementation

The simulator tracks the **Draft Acceptance Rate ($\alpha$)**.

* **Draft Step:** Run forward pass using $W_{draft}$.
* **Confidence Check:** Calculate Logit Margin. If $\text{Margin} < \text{Threshold}$, reject immediately (simulate fallback to Verify Mode).
* **Verification:** If accepted by logic, compare Draft Token vs. Ground Truth Token (from full model). Record False Positives and True Positives.

---

### 5.3 Part B: Hardware Performance Estimator (The Calculator)

We will develop a configurable Python calculator to estimate physical PPA (Power, Performance, Area). This tool inputs the trace from Part A and outputs total Energy/Latency.

#### 5.3.1 Hardware Specifications Input

We require specific numbers from literature to populate the calculator.

| Component | Specification Parameter | Value Source (Reference) |
| :--- | :--- | :--- |
| **Memristor Array** | Read Energy/bit ($E_{cell}$) | **[PLACEHOLDER: e.g., ISSCC 2023 ReRAM Paper]** |
| **Memristor Array** | Read Latency ($T_{cell}$) | **[PLACEHOLDER]** |
| **ADC-A (Draft)** | 4-bit Flash ADC Power/Time | **[PLACEHOLDER: e.g., Murmann ADC Survey]** |
| **ADC-B (Residual)** | 12-bit SAR/Pipe ADC Power/Time | **[PLACEHOLDER]** |
| **DAC** | Predictive Bias DAC Power | **[PLACEHOLDER]** |
| **Digital Logic** | Adder/Accumulator Power (28nm) | **[PLACEHOLDER]** |

#### 5.3.2 Metric Formulation

The calculator will compute Total Energy ($E_{total}$) using the **Conditional Activation** formula:

$$ E_{total} = N_{tokens} \times [ \underbrace{(E_{Arr1} + E_{ADC\_A})}_{\text{Always Active}} + \underbrace{(1 - \alpha) \times (E_{Arr2-4} + E_{ADC\_B})}_{\text{Active only on Reject}} ] $$

* Where $\alpha$ is the Acceptance Rate determined by the PyTorch simulation.

---

### 5.4 Design Space Exploration (Experiments)

We will use the simulator to sweep across configurable parameters to find the optimal Hardware-Algorithm configuration.

#### Experiment 1: Optimal ADC Resolution Split

We will test different bit-width allocations for the "1+3 Split" to balance draft quality vs. energy.

* **Test Variable:** Resolution of ADC-A (Draft) vs. ADC-B (Residual).
* **Configurations to Sweep:**
    1. ADC-A: **3-bit**, ADC-B: **13-bit** (Lowest Draft Energy, Low Acceptance).
    2. ADC-A: **4-bit**, ADC-B: **12-bit** (Balanced).
    3. ADC-A: **5-bit**, ADC-B: **11-bit** (High Acceptance, Higher Base Energy).
* **Optimization Target:** Maximize Tokens/Joule.

#### Experiment 2: Noise Tolerance & Fine-Tuning Impact

Quantify the benefit of the "Noise-Injection Adaptation" (Algorithm Opt 2).

* **Test Variable:** Analog Write Noise Std Dev ($\sigma$).
* **Comparison:**
  * Baseline Model (Zero-shot).
  * Noise-Tuned Model (Fine-tuned with $\sigma$).
* **Goal:** Demonstrate that Fine-Tuning maintains $>$85% Acceptance Rate even as hardware noise increases (e.g., up to $\sigma = 0.05$).

#### Experiment 3: Layer Sensitivity Map

Determine which layers require "1-Array Draft" vs "2-Array Draft" (Algorithm Opt 1).

* **Procedure:** Bruteforce sweep. Set one layer to "Draft Mode" and measure Perplexity drop.
* **Output:** A heat map of layer sensitivity, establishing the static lookup table for the generic Draft Allocation policy.
